{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TDE 2 - Spark.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Grupo:\n","\n","\n","\n","*   Carlos Henrique dos Santos\n","*   Gabriel Scholze Rosa\n","*   João Vitor Brandão\n","*   Matheus Leindorf Muller\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"QgSugfsa0tq1"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmNE_7dwiUHr","executionInfo":{"status":"ok","timestamp":1653594364027,"user_tz":180,"elapsed":52828,"user":{"displayName":"Matheus Muller","userId":"15529390735415578446"}},"outputId":"e009740b-836d-48c0-e170-528eb0ee2f0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 63.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=9141d2add7b7a356208392a4aa1604b1656a6a624a24dacba9392c1fd457112f\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"]}]},{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark import SparkFiles\n","\n","spark = SparkSession.builder.master('local[*]').\\\n","        appName('Muller').getOrCreate()\n","sc = spark.sparkContext"],"metadata":{"id":"wr4-i6SJOG0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sc.addFile('/content/drive/MyDrive/transactions_amostra.csv')"],"metadata":{"id":"SBAZjYItOG2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd = sc.textFile('file://' + SparkFiles.get('transactions_amostra.csv'))"],"metadata":{"id":"ccKspdTlOG5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# slip de sentenças em palavras\n","palavras = rdd.map(lambda frase: frase.split(';')).filter(lambda x: x[1].isnumeric()) "],"metadata":{"id":"5krktbnGOG7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 1\n","# We did a filter to select only \"Brazil\" between the countries, then we utilized a map to add \"1\" for each \"Brazil\" found,\n","# Finnaly, We did a ReduceByKey to sum the values by key. \n","palavras.filter(lambda x: x[0]=='Brazil').map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y).coalesce(1).saveAsTextFile('Questao1')"],"metadata":{"id":"s28S2RGGONqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 2\n","# We did a map to add \"1\" for each year found, then a ReduceByKey was made to sum all the different years.\n","palavras.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x+y).coalesce(1).saveAsTextFile('Questao2')\n"],"metadata":{"id":"GznC2w_yONsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 3\n","# We did a map to add \"1\" for each year and flow type found, then a ReduceByKey was made to sum all the diferrent occorence of years and flow types\n","palavras.map(lambda x: (x[1] + \" \" + x[4], 1)).reduceByKey(lambda x, y: x+y).coalesce(1).saveAsTextFile('Questao3')"],"metadata":{"id":"zuht4sPxONu1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 4\n","# We did a map to join the year, price and a \"1\" to count later. Then we did a ReduceByKey to sum all the prices, and count them. \n","# Finally, we made a MapValues to divide the sum of the prices by their counting (Making the Avarage).\n","palavras.map(lambda x: (x[1], (float(x[5]),1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).mapValues(lambda x: x[0]/x[1]).coalesce(1).saveAsTextFile('Questao4')"],"metadata":{"id":"XRwlfEv3BLc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 5\n","# We did a filter to select only \"Brazil\" and \"Export\" between the countries and flow types. Then we did a map to join the year, unit and category as key; Price as value and a \"1\" to count later.\n","# Then we did a ReduceByKey  to sum all the prices, and count them. Finally, we made a MapValues to divide the sum of the prices by their counting (Making the Avarage).\n","palavras.filter(lambda x: x[0]==\"Brazil\" and x[4]==\"Export\").map(lambda x: (x[1]+\" \"+ x[7]+ \" \"+x[9],(float(x[5]),1)))\\\n",".reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).mapValues(lambda x: x[0]/x[1]).coalesce(1).saveAsTextFile('Questao5')"],"metadata":{"id":"7hPnmXWSi5kG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 6\n","# We did a map to join the unit and the year as a key, and 3 prices for different operations: the maximum, the minimum and the mean as values, and a \"1\" to count later. \n","# Then we did a ReduceByKey to get the maximum, the minimum and the mean prices. Finally we made a map to put the items in order, and get the mean by dividing the sum of prices by the count of them.\n","palavras.map(lambda x: (x[7] + \" \"+x[1],(float(x[5]),float(x[5]),float(x[5]),1.0)))\\\n",".reduceByKey(lambda x,y: (x[0] if x[0]>y[0] else y[0],\\\n","                          x[1] if x[1]<y[1] else y[1],\\\n","                           x[2]+y[2], x[3]+y[3]))\\\n","                           .map(lambda x: (x[0], (x[1][0],x[1][1], x[1][2]/x[1][3]))).coalesce(1).saveAsTextFile('Questao6')"],"metadata":{"id":"XJ81s84si5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questao 7\n","# Fisrt we did a filter to select only \"2016\" between the years. Then we did a map to join the Flow with the Commodity as key, and the Amount as Value. \n","# Then we did a GroupByKey to organize and another map to get the Flow type as key, and the sum of Amount with Commodity as Values.\n","# Finally we did a ReduceByKey to get only the most commercialized commodity in 2016 by Flow Type\n","palavras.filter(lambda x: x[1]==\"2016\")\\\n",".map(lambda x: ((x[4],x[3]),float(x[8])))\\\n",".groupByKey().map(lambda x: (x[0][0], (sum(x[1]),x[0][1])))\\\n",".reduceByKey(lambda x,y: x if x>y else y).coalesce(1).saveAsTextFile('Questao7')"],"metadata":{"id":"acXOLb2Pi59X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Grupo:\n","\n","\n","\n","*   Carlos Henrique dos Santos\n","*   Gabriel Scholze Rosa\n","*   João Vitor Brandão\n","*   Matheus Leindorf Muller"],"metadata":{"id":"V4Kije8k4gSh"}}]}